{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c782d0aa",
   "metadata": {
    "id": "w2X86rW6m76p"
   },
   "source": [
    "# Training a causal language model from scratch (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87997cf",
   "metadata": {
    "id": "do0eMizIm76r"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2658f0-16d5-4df4-b823-83279f25bb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create python 3.10 greedyLR virtual env\n",
    "\n",
    "# 1. git clone https://github.com/balak4/transformers/tree/main /home/ec2-user/SageMaker/transformers\n",
    "# 2. conda env create -f git/balak4/transformers/examples/greedy-lr/conda/pytorch_p310_greedy_v2.yml\n",
    "# 3. Install modified transformers fork in local env:\n",
    "#     1. source ~/.bashrc\n",
    "#     2. conda activate py310-greedy\n",
    "#     3. python3 -m pip install -e /home/ec2-user/SageMaker/git/balak4/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7942d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"pytorch\", torch.__version__)\n",
    "print(\"transformers\", transformers.__version__)\n",
    "print(\"accelerate\", accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4551e8ef-f6c8-4650-baad-978b4fbc6fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SET SEED\n",
    "RANDOM_SEED = 42\n",
    "transformers.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fec57dbe",
   "metadata": {
    "id": "wHEQUuI1m76s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27d3d750",
   "metadata": {
    "id": "snW5SGvUm76s",
    "outputId": "3af8b4fc-00aa-4f86-b95a-4438ae3daba2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False True\n"
     ]
    }
   ],
   "source": [
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "436e6753",
   "metadata": {
    "id": "vqoy2Z3cm76t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021098b",
   "metadata": {
    "id": "ON74kPgEm76t",
    "outputId": "2f91a0e8-7bde-44f0-958d-3b4c7a49e2b6"
   },
   "outputs": [],
   "source": [
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# split = \"train\"  # \"valid\"\n",
    "# filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "# data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "# filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeb1467",
   "metadata": {
    "id": "Ow4ZjCN_m76t",
    "outputId": "34fbb66a-c358-48a9-f7a1-148cad902803",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 8.25G/8.25G [02:11<00:00, 62.7MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f405c59749cd48df890446a512e908e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 46.1M/46.1M [00:00<00:00, 63.4MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f793b1a91c4a9da739032226825822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 606720\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 3322\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        \"train\": ds_train,  # .shuffle().select(range(50000)),\n",
    "        \"valid\": ds_valid,  # .shuffle().select(range(500))\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "996bde60-3d09-4f5f-ba20-d522e6ae7040",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['repo_name', 'path', 'copies', 'size', 'content', 'license'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test - small data batch\n",
    "\n",
    "# raw_datasets_small = DatasetDict(\n",
    "#     {\n",
    "#         \"train\": ds_train.shuffle().select(range(50_000)),\n",
    "#         \"valid\": ds_valid.shuffle().select(range(500)),\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# raw_datasets_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44813819",
   "metadata": {
    "id": "80v0UaGHm76t",
    "outputId": "3c1d63b1-b19b-426c-d174-6a8957fe3eb8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_NAME: kmike/scikit-learn\n",
      "PATH: sklearn/utils/__init__.py\n",
      "COPIES: 3\n",
      "SIZE: 10094\n",
      "CONTENT: \"\"\"\n",
      "The :mod:`sklearn.utils` module includes various utilites.\n",
      "\"\"\"\n",
      "\n",
      "from collections import Sequence\n",
      "\n",
      "import numpy as np\n",
      "from scipy.sparse import issparse\n",
      "import warnings\n",
      "\n",
      "from .murmurhash import murm\n",
      "LICENSE: bsd-3-clause\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa5510d",
   "metadata": {
    "id": "1ny1lF9fm76t",
    "outputId": "82c32afb-e938-4911-85f0-d908c1d0c257",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6411975b47049a691a5a3e439d95a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/265 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f072ef72e94696a8600e6fb0e6d2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/789k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a2939f4e924d5ebe01e7bc35901d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/448k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7fb8c95d4942919813087f6b3272fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a0433ff63c465f9d83de660c83e794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 34\n",
      "Input chunk lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 117, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 41]\n",
      "Chunk mapping: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79dc7be8-40f6-4d59-8ae9-545ab988a8ad",
   "metadata": {
    "id": "kXICgDESm76t",
    "outputId": "6b838335-64aa-4db6-a073-7a774587b677",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3b53535b-584d-45f5-ac47-f8f253f1899b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50000/50000 [01:54<00:00, 438.17 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:01<00:00, 461.05 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 1383736\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 13310\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test - small\n",
    "\n",
    "# tokenized_datasets = raw_datasets_small.map(\n",
    "#     tokenize, batched=True, remove_columns=raw_datasets_small[\"train\"].column_names\n",
    "# )\n",
    "# tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba80953-0cc7-48e8-bd62-ed289ed9791c",
   "metadata": {
    "id": "kXICgDESm76t",
    "outputId": "6b838335-64aa-4db6-a073-7a774587b677",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912eb84d4cdd4951b670653125a87681",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606720 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8681ca1fca43476da0fa17bff426b277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3322 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 16702061\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 93164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20310f72-b0d3-4e66-9625-ae947353be8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbef471331fc4df18c033c01364c8b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/18 shards):   0%|          | 0/16702061 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f345973f8c848e58aa47624af3e3716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/93164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_name = 'tokenized_dataset_full'\n",
    "save_dir = f'./logs/codeparrot-ds/{dataset_name}/'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tokenized_datasets.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d24b6d8c-0d93-4ada-9f11-71f7442572a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reduce size of tokenized datasets\n",
    "\n",
    "from datasets import DatasetDict\n",
    "\n",
    "half_size_datasets = DatasetDict({\n",
    "    split_name: dataset.shuffle(seed=42).select(range(len(dataset) // 2))\n",
    "    for split_name, dataset in tokenized_datasets.items()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe809607-8d7f-4f4e-a3cc-26d711ddd319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 16702061\n",
      "Original valid size: 93164\n",
      "New train size: 8351030\n",
      "New valid size: 46582\n"
     ]
    }
   ],
   "source": [
    "# Print original sizes\n",
    "for split_name, dataset in tokenized_datasets.items():\n",
    "    print(f\"Original {split_name} size:\", len(dataset))\n",
    "\n",
    "# Print new sizes\n",
    "for split_name, dataset in half_size_datasets.items():\n",
    "    print(f\"New {split_name} size:\", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "efe6bc72-b97a-4df1-8338-af52bbd11077",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1cb1c3294144be9ded26b635be6caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/9 shards):   0%|          | 0/8351030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f0b4fd0d314d389ec807ada16ecd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/46582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = 'tokenized_dataset_half_seed_42'\n",
    "save_dir = f'./logs/codeparrot-ds/{dataset_name}/'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "half_size_datasets.save_to_disk(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04a0fab6-e242-4efd-8382-4d7604a2c1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-assign dataset to re-use variable names below\n",
    "tokenized_datasets = half_size_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ab57a24-3863-4453-b8d9-cf502e8ee9bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 8351030\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 46582\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d50be76c",
   "metadata": {
    "id": "SkkLhp2sm76u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cc5864-a2f5-4ae8-ab16-6a3b3a52f413",
   "metadata": {},
   "source": [
    "## LR SCHEDULER - GREEDYLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3ebd4a6",
   "metadata": {
    "id": "OqCZGGpnm76u",
    "outputId": "0dda6666-1251-4b02-c5b5-4dafcf307b73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.2M parameters\n"
     ]
    }
   ],
   "source": [
    "# model = GPT2LMHeadModel(config)\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c9a683ec",
   "metadata": {
    "id": "x58L1EN3m76u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb09eb6d",
   "metadata": {
    "id": "oefxlrpum76u",
    "outputId": "2d3397f2-5c9a-46fe-e356-6495dd7dcbd1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([5, 128])\n",
      "attention_mask shape: torch.Size([5, 128])\n",
      "labels shape: torch.Size([5, 128])\n"
     ]
    }
   ],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55b643a0-fc8c-4053-962e-4f15cab5209e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./logs/codeparrot-ds/run2/greedylr/2025-01-25/tensorboard\n",
      "./logs/codeparrot-ds/run2/greedylr/2025-01-25/output\n",
      "./logs/codeparrot-ds/run2/greedylr/2025-01-25/pretrained-model-dir\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"codeparrot-ds\"\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "run_num = 2\n",
    "run_name = \"greedylr\"\n",
    "\n",
    "logging_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/tensorboard\"\n",
    "output_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/output\"\n",
    "model_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/pretrained-model-dir\"\n",
    "\n",
    "print(logging_dir)\n",
    "print(output_dir)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47263361",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# greedyLR\n",
    "train_args = TrainingArguments(\n",
    "    per_device_train_batch_size=48, # 32, 48\n",
    "    per_device_eval_batch_size=48, # 32, 48\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    bf16=False, # Use to lower memory requirement\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    warmup_steps=1_000,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"tensorboard\",\n",
    "    lr_scheduler_type=\"greedy\",\n",
    "    # greedy\n",
    "    min_lr=1.85e-05,\n",
    "    smooth=True,\n",
    "    factor=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b56fa98-a4ed-479b-a54a-9aaa021711e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator, DataLoaderConfiguration\n",
    "\n",
    "# For accelerate = 0.28.0\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# For newer versions of accelerate, e.g. accelerate = 1.3.0\n",
    "# Define DataLoaderConfiguration\n",
    "# dataloader_config = DataLoaderConfiguration(\n",
    "#     dispatch_batches=False,  # Each process fetches its own batch\n",
    "#     split_batches=True       # Split fetched batches across processes\n",
    "# )\n",
    "\n",
    "# # Initialize Accelerator with DataLoaderConfiguration\n",
    "# accelerator = Accelerator(dataloader_config=dataloader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b2473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedyLR settings: patience=10 smooth=True min_lr=1.85e-05 factor=0.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3573' max='5436' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3573/5436 8:29:33 < 4:25:50, 0.12 it/s, Epoch 0.66/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.456800</td>\n",
       "      <td>2.357604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.862900</td>\n",
       "      <td>1.784082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.616300</td>\n",
       "      <td>1.529091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.481400</td>\n",
       "      <td>1.419977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.452100</td>\n",
       "      <td>1.386506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.446300</td>\n",
       "      <td>1.378297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.446000</td>\n",
       "      <td>1.371534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/SageMaker/conda/envs/py310-greedy/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "trainer = accelerator.prepare(Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    args=train_args,\n",
    "    data_collator=data_collator\n",
    "))\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\n",
    "    model_dir,\n",
    "    safe_serialization=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ad807-7e55-45fd-a054-93fcdcfcdc34",
   "metadata": {},
   "source": [
    "## LR SCHEDULER: COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c16a5-83db-4e4c-8115-11149a8f3b94",
   "metadata": {
    "id": "OqCZGGpnm76u",
    "outputId": "0dda6666-1251-4b02-c5b5-4dafcf307b73",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# INITALIZE MODEL\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70dae0-f458-4676-be46-ad690ec2d7fd",
   "metadata": {
    "id": "x58L1EN3m76u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f6022-9810-469f-bd38-cb2e72dce74c",
   "metadata": {
    "id": "oefxlrpum76u",
    "outputId": "2d3397f2-5c9a-46fe-e356-6495dd7dcbd1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f35bcb-a713-4aa8-a882-730bf2d6962e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_name = \"codeparrot-ds\"\n",
    "date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "run_num = 2\n",
    "run_name = \"cosine\"\n",
    "\n",
    "logging_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/tensorboard\"\n",
    "output_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/output\"\n",
    "model_dir = f\"./logs/{exp_name}/run{run_num}/{run_name}/{date}/pretrained-model-dir\"\n",
    "\n",
    "print(logging_dir)\n",
    "print(output_dir)\n",
    "print(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20ba12",
   "metadata": {
    "id": "WhElFKACm76u",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Orginal: cosine\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    per_device_train_batch_size=48,  # 16, 8\n",
    "    per_device_eval_batch_size=48,\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    weight_decay=0.1,\n",
    "    bf16=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    warmup_steps=1_000,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"tensorboard\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = accelerator.prepare(Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=train_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\n",
    "    model_dir,\n",
    "    safe_serialization=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b1737",
   "metadata": {
    "id": "WOhCvDEEm76u"
   },
   "outputs": [],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c50b5c7-2847-49c0-93da-2f82305d6b1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Model Inference ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a60c3a",
   "metadata": {
    "id": "OF2zZNwNm76u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"huggingface-course/codeparrot-ds\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a659fcc7",
   "metadata": {
    "id": "ITopE3Clm76u",
    "outputId": "e8ca8a7c-e659-4672-f995-63a717d28ddf"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb86877",
   "metadata": {
    "id": "wwfw2wIPm76u",
    "outputId": "315dcc01-2441-4ca3-ba9c-bdc04c66cd5d"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a17ce",
   "metadata": {
    "id": "evUq3PUum76u",
    "outputId": "cd160150-7471-45e4-aaa5-2fb816935683"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83958a83",
   "metadata": {
    "id": "-WoJ3LrRm76u",
    "outputId": "4df32ee8-4d1b-477c-c121-d8d638347b44"
   },
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c7f7f8-58d1-4091-b633-26ac1f32a9a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Sample Training Loop with Accelerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e1321e",
   "metadata": {
    "id": "M8DH_K8Bm76u",
    "outputId": "f5018f1f-53bc-4562-d548-c3b529b3d683"
   },
   "outputs": [],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415a7d5",
   "metadata": {
    "id": "YhOxzzfOm76u"
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc6356",
   "metadata": {
    "id": "Z6Upkd8_m76u"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce38bfc",
   "metadata": {
    "id": "q7h25Q7lm76u"
   },
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da109eac",
   "metadata": {
    "id": "C0Kei_sgm76u"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c0689",
   "metadata": {
    "id": "K3QfRmNKm76v"
   },
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329afd",
   "metadata": {
    "id": "AzjiT25Bm76v"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78079c53",
   "metadata": {
    "id": "Nox7uzFlm76v"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24f8ec",
   "metadata": {
    "id": "OMoRDmd4m76v"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2803b",
   "metadata": {
    "id": "8y2QdyvIm76v",
    "outputId": "d516603a-d817-4e7a-d347-10b42988339e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"codeparrot-ds-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616866e",
   "metadata": {
    "id": "IwiWVK8cm76v"
   },
   "outputs": [],
   "source": [
    "output_dir = \"codeparrot-ds-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb2e9c",
   "metadata": {
    "id": "SE5s4wslm76y",
    "outputId": "e9e6dff5-e922-4dbc-e4a2-49db9cdb1272"
   },
   "outputs": [],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a6b42",
   "metadata": {
    "id": "puJDBNqdm76y"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5_000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"lr\": get_lr(),\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a causal language model from scratch (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py310-greedy",
   "language": "python",
   "name": "py310-greedy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
