# ğŸ› ï¸ Training Debugging & Testing Guide

> **SageMaker Studio Note**: This guide is optimized for SageMaker Studio environments running on g5.12xlarge or larger instances. NCCL timeout issues are particularly relevant for multi-GPU setups in SageMaker environments due to their network configuration.

## ğŸš¨ Problem Solved

**NCCL Timeout Error** during distributed training has been resolved with comprehensive fixes:

### âŒ Original Error
```
NCCL watchdog thread terminated with exception: WorkNCCL timeout
Operation _ALLGATHER_BASE ran for 24960814 milliseconds before timing out
```

### âœ… Solution Implemented
1. **NCCL timeout increased** to 1 hour (from 30 minutes)
2. **Evaluation batch size reduced** to prevent memory pressure
3. **Evaluation frequency reduced** (500 steps instead of 200)
4. **Additional stability optimizations** added

---

## ğŸš€ Quick Start

### 1. Test Training Stability (Recommended First)
```bash
cd examples/greedy-lr
conda activate pytorch_p310_greedy_v2

# Run comprehensive stability tests
python test_training_stability.py

# Test only specific scheduler
python test_training_stability.py --schedulers cosine
```

### 2. Run Full Training (After tests pass)
```bash
# Sequential training with both schedulers
./run_sequential_training.sh

# Monitor training progress (in separate terminal)
python monitor_training.py
```

---

## ğŸ”§ What Was Fixed

### **Main Training Script (`pre-train-llama3.2-1b.py`)**

#### **NCCL Optimizations**
```python
# Extended timeout for NCCL operations
os.environ["NCCL_TIMEOUT"] = "3600"  # 1 hour

# Stability improvements
os.environ["NCCL_IB_DISABLE"] = "1"      # Disable InfiniBand
os.environ["NCCL_P2P_DISABLE"] = "1"     # Disable P2P
os.environ["NCCL_TREE_THRESHOLD"] = "0"  # Force ring algorithm
```

#### **Training Configuration Changes**
```python
# Before (causing timeouts)
"per_device_eval_batch_size": 2,
"eval_steps": 200,

# After (stable)
"per_device_eval_batch_size": 1,    # Reduced memory pressure
"eval_steps": 500,                  # Less frequent evaluation
"dataloader_num_workers": 0,        # More stable data loading
"prediction_loss_only": True,       # Faster evaluation
```

### **Testing Framework (`test_training_stability.py`)**

Progressive testing to validate:
- âœ… Basic training loop (10 steps)
- âœ… Training with evaluation (50 steps)  
- âœ… Training with checkpointing (50 steps)
- âœ… Full features enabled (100 steps)
- âœ… Extended run (500 steps)

---

## ğŸ“Š Testing Phases

The stability tester runs **5 progressive tests** for each scheduler:

| Test | Steps | Eval | Save | Purpose |
|------|-------|------|------|---------|
| **basic** | 10 | âŒ | âŒ | Validate core training loop |
| **with_eval** | 50 | âœ… | âŒ | Test evaluation stability |
| **with_checkpoints** | 50 | âŒ | âœ… | Test checkpointing |
| **full_features** | 100 | âœ… | âœ… | Test complete functionality |
| **extended** | 500 | âœ… | âœ… | Extended stability test |

**Early Stop Logic**: If any test fails, subsequent tests are skipped for that scheduler.

---

## ğŸ“ˆ Monitoring

### **Real-time Training Monitor**
```bash
# Start monitoring (updates every 30 seconds)
python monitor_training.py

# Faster updates during critical phases
python monitor_training.py --refresh 15
```

**What you'll see:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ğŸ¦™ LLaMA Training Monitor                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Phase: Greedy LR Training                                          â•‘
â•‘ Progress: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 847/2000 (42.4%)      â•‘
â•‘ Time: 2h 15m elapsed | Est. remaining: 2h 45m                     â•‘
â•‘                                                                    â•‘
â•‘ Current Metrics:                                                   â•‘
â•‘   Training Loss: 3.2847 â¬‡ï¸ (improving)                           â•‘
â•‘   Learning Rate: 1.95e-04                                          â•‘
â•‘   Grad Norm: 0.8921 â¬‡ï¸ (improving)                               â•‘
â•‘                                                                    â•‘
â•‘ [Press Ctrl+C to exit]                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ” Troubleshooting

### **If Tests Fail**
1. **Check GPU memory**: `nvidia-smi`
2. **Review test logs**: `training_stability_test.log`
3. **Examine detailed report**: `test_runs/stability_test_report.json`

### **Common Issues & Solutions**

#### **"CUDA out of memory"**
```python
# Reduce batch sizes in pre-train-llama3.2-1b.py
"per_device_train_batch_size": 1,  # From 2
"per_device_eval_batch_size": 1,   # Already set
"gradient_accumulation_steps": 64, # Increase to maintain effective batch size
```

#### **"Dataset not found"**
```bash
# Ensure dataset is prepared
ls -la datasets/redpajama/redpajama_50K_seed_42/
```

#### **"NCCL still timing out"**
```python
# Increase timeout further
os.environ["NCCL_TIMEOUT"] = "7200"  # 2 hours
```

---

## ğŸ“ File Structure

```
examples/greedy-lr/
â”œâ”€â”€ pre-train-llama3.2-1b.py          # âœ… Fixed training script
â”œâ”€â”€ test_training_stability.py         # ğŸ§ª Stability testing framework  
â”œâ”€â”€ monitor_training.py                # ğŸ“Š Real-time training monitor
â”œâ”€â”€ run_sequential_training.sh         # ğŸš€ Sequential training runner
â”œâ”€â”€ DEBUGGING_GUIDE.md                # ğŸ“– This guide
â””â”€â”€ test_runs/                         # ğŸ“‚ Test outputs
    â”œâ”€â”€ stability_test_report.json     # ğŸ“„ Detailed test results
    â””â”€â”€ training_stability_test.log    # ğŸ“„ Test execution log
```

---

## âœ… Success Criteria

**Before running 2000-step training:**
1. âœ… All stability tests pass
2. âœ… Monitor shows consistent progress
3. âœ… No NCCL timeout errors in short tests
4. âœ… GPU memory usage stable

**Training should now complete successfully without NCCL timeouts!**

---

## ğŸ¯ Next Steps

1. **Run stability tests**: `python test_training_stability.py`
2. **If tests pass**: `./run_sequential_training.sh`
3. **Monitor progress**: `python monitor_training.py` (separate terminal)
4. **Compare results**: Use existing comparison tools after completion

The comprehensive fixes address the root causes of NCCL timeouts and provide robust testing to ensure training stability.
